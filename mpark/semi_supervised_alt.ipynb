{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mmm\\Projects\\ont_m6a\\mpark\\semi_supervised_alt.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mmm/Projects/ont_m6a/mpark/semi_supervised_alt.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mmm/Projects/ont_m6a/mpark/semi_supervised_alt.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mmm/Projects/ont_m6a/mpark/semi_supervised_alt.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(tf\u001b[39m.\u001b[39mversion\u001b[39m.\u001b[39mVERSION)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mmm/Projects/ont_m6a/mpark/semi_supervised_alt.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)\n",
    "from tensorflow import keras\n",
    "# from tf.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout, MaxPooling1D, Conv1D, Flatten\n",
    "# from tensorflow.keras.metrics import AUC\n",
    "\n",
    "tf\n",
    "\n",
    "class M6ALstm(tf.keras.Sequential):\n",
    "    def __init__(\n",
    "            self, num_hidden_layers=1, num_units=32, dropout_rate=0.1,\n",
    "            activation_function='relu', learning_rate=0.05, momentum_value=0.7,\n",
    "            batch_size=32, num_nucleotides=7\n",
    "    ):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        self.num_nucleotides = num_nucleotides\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.momentum_value = momentum_value\n",
    "\n",
    "        full_window = num_nucleotides * 2 + 1\n",
    "        input_shape = ((full_window), 5)\n",
    "\n",
    "        for _ in range(num_hidden_layers):\n",
    "            if _ == 0:\n",
    "                self.add(LSTM(num_units, input_shape=input_shape, return_sequences=(num_hidden_layers > 1)))\n",
    "            else:\n",
    "                self.add(LSTM(num_units, return_sequences=(_ < num_hidden_layers)))\n",
    "\n",
    "            self.add(Dropout(dropout_rate))\n",
    "\n",
    "        self.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_set_by_base_quality(features, labels, quality_threshold=0.05, drop_base_quality=True):\n",
    "\n",
    "        list_above_threshold = []\n",
    "        \n",
    "        for i in range(features.shape[0]):\n",
    "            if features[i, 4, 7] > quality_threshold:\n",
    "                list_above_threshold.append(i)\n",
    "        \n",
    "        above_threshold_indices = np.array(list_above_threshold)\n",
    "        \n",
    "        above_threshold_features = features[above_threshold_indices]\n",
    "        above_threshold_labels = labels[above_threshold_indices]\n",
    "        \n",
    "        if drop_base_quality:\n",
    "            above_threshold_features = above_threshold_features[:, np.arange(above_threshold_features.shape[1]) != 4, :]\n",
    "        \n",
    "        return above_threshold_features, above_threshold_labels\n",
    "\n",
    "    @staticmethod\n",
    "    def set_num_nucleotides(features, num = 7):\n",
    "        if num > 7:\n",
    "            print('Cannot have more than 7 nucleotides.')\n",
    "        else:\n",
    "            num_to_remove = 7 - num\n",
    "            \n",
    "            keep_from_start = num_to_remove\n",
    "            keep_from_end = features.shape[2] - num_to_remove\n",
    "\n",
    "            new_features = features[:, :, keep_from_start:keep_from_end]\n",
    "            \n",
    "            return new_features\n",
    "    \n",
    "    def fit_semisupervised(\n",
    "            self,\n",
    "            train_features=None,\n",
    "            train_labels=None,\n",
    "            val_features=None,\n",
    "            val_labels=None,\n",
    "            max_epochs=10,\n",
    "            device=\"cpu\",\n",
    "            best_save_model=\"\",\n",
    "            final_save_model=\"\",\n",
    "            prev_aupr=0,\n",
    "    ):\n",
    "        # filter train set\n",
    "        train_features, y_train = self.filter_set_by_base_quality(train_features, train_labels)\n",
    "        train_features = self.set_num_nucleotides(train_features, self.num_nucleotides)\n",
    "        X_train = train_features.transpose((0, 2, 1))\n",
    "\n",
    "        # filter val set    \n",
    "        val_features, y_val = self.filter_set_by_base_quality(val_features, val_labels)\n",
    "        val_features = self.set_num_nucleotides(val_features, self.num_nucleotides)\n",
    "        X_val = val_features.transpose((0, 2, 1)) \n",
    "        print(X_val.shape, y_val.shape)\n",
    "\n",
    "        optimizer = Adam(learning_rate=self.learning_rate, beta_1=self.momentum_value)\n",
    "        self.compile(\n",
    "            loss='binary_crossentropy', \n",
    "            optimizer=optimizer, \n",
    "            metrics=['accuracy', 'precision', 'recall', AUC(curve='PR')]  \n",
    "            # Average precision is the area under the PR curve\n",
    "        )\n",
    "        for epoch in range(max_epochs):\n",
    "            history = self.fit(X_train, y_train, batch_size=self.batch_size, validation_data=(X_val, y_val), verbose=0)\n",
    "            val_accuracy = history.history['val_accuracy'][-1]\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{max_epochs}, \" \n",
    "                f\"Val Acc: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import configparser\n",
    "import _pickle as pickle\n",
    "#from torchsummary import summary\n",
    "#from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from tensorflow.keras.metrics import AUC, Accuracy\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from m6a_lstm import M6ALstm\n",
    "\n",
    "# EP assumption: for every non m6a call above threshold, there is a false positive call\n",
    "# precision: ratio of true positives to total number of positive calls\n",
    "# FDR: proportion of positives called from the clean negative set\n",
    "# mixed set is cleaned to be \"postiive\" when entering the CNN\n",
    "\n",
    "verbose = False\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "def count_pos_neg(labels, set_name=\"\"):\n",
    "    \"\"\"\n",
    "    Count the number of positive\n",
    "    (m6A) and negative (everything\n",
    "    else) examples in our set\n",
    "    :param labels: np.array, one-hot-encoded\n",
    "                             label,\n",
    "    :param set_name: str, training,\n",
    "                          validation or\n",
    "                          test set.\n",
    "    :return: #positives, #negatives\n",
    "    \"\"\"\n",
    "    # First column are positive labels\n",
    "    m6as = np.where(labels == 1)[0]\n",
    "    # Second column are negative labels\n",
    "    nulls = np.where(labels == 0)[0]\n",
    "    num_pos = len(m6as)\n",
    "    num_neg = len(nulls)\n",
    "    if verbose:\n",
    "        print(f\"{set_name} has {num_pos}\" f\" positives and {num_neg} negatives\")\n",
    "    return num_pos, num_neg\n",
    "\n",
    "\n",
    "def make_one_hot_encoded(y_array):\n",
    "    \"\"\"\n",
    "    Convert int labels to one\n",
    "    hot encoded labels\n",
    "    :param y_array: np.array, int labels\n",
    "    :return: one-hot-encoded labels\n",
    "    \"\"\"\n",
    "    y_array_ohe = np.zeros((len(y_array), 2))\n",
    "    one_idx = np.where(y_array == 1)[0] #indices with value=1\n",
    "    y_array_ohe[one_idx, 0] = 1 # col0: indices w pos label\n",
    "\n",
    "    zero_idx = np.where(y_array == 0)[0] #indices with value=0\n",
    "    y_array_ohe[zero_idx, 1] = 1 # col2: indices w neg label\n",
    "    return y_array_ohe \n",
    "\n",
    "\n",
    "# class M6ADataGenerator(torch.utils.data.Dataset):\n",
    "#     \"\"\"\n",
    "#     Data generator for the m6A\n",
    "#     model. It randomly selects\n",
    "#     batches from the data to\n",
    "#     train the m6A model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, features, labels):\n",
    "#         \"\"\"\n",
    "#         Constructor for the data\n",
    "#         generator class, expects\n",
    "#         features and labels matrices.\n",
    "#         :param features: numpy.array,\n",
    "#                             Nx15, N=number of\n",
    "#                             sequences, each\n",
    "#                             sequence is of\n",
    "#                             length 15.\n",
    "#         :param labels: numpy array,\n",
    "#                             one-hot-encoded\n",
    "#                             labels for whether\n",
    "#                             a sequence in\n",
    "#                             features variable\n",
    "#                             contains methylated\n",
    "#                             A or not.\n",
    "#         \"\"\"\n",
    "#         self.features = features\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Get random indices from\n",
    "#         the training data\n",
    "#         to form batches.\n",
    "#         :param idx: numpy.array,\n",
    "#                     indices to retrieve.\n",
    "#         :return: x, y: features and\n",
    "#                        labels from\n",
    "#                        selected indices.\n",
    "#         \"\"\"\n",
    "#         x = self.features[idx]\n",
    "#         y = self.labels[idx]\n",
    "\n",
    "#         x = torch.tensor(x)\n",
    "#         y = torch.tensor(y)\n",
    "\n",
    "#         return x, y\n",
    "\n",
    "\n",
    "def m6AGenerator(\n",
    "    train_path,\n",
    "    val_path,\n",
    "    input_size,\n",
    "    random_state=None,\n",
    "    train_sample=True,\n",
    "    train_sample_fraction=0.5,\n",
    "    val_sample=True,\n",
    "    val_sample_fraction=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    This generator returns a training\n",
    "    data generator as well as validation\n",
    "    features and labels.\n",
    "    :param train_path: str, path where\n",
    "                            the training\n",
    "                            data is stored.\n",
    "    :param val_path: str, path where the\n",
    "                          val data is stored.\n",
    "    :param input_size: int, number of input channels\n",
    "    :param random_state: np.random, random seed\n",
    "    :param train_sample: bool, sample train data\n",
    "    :param train_sample_fraction: float, what fraction\n",
    "                                         to sample\n",
    "    :param val_sample: bool, sample validation data\n",
    "    :param val_sample_fraction: float, what fraction\n",
    "                                       to sample\n",
    "    :return: X_gen: training data generator,\n",
    "             X_val: validation features,\n",
    "             y_val: validation labels\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    train_data = np.load(train_path, allow_pickle=True)\n",
    "    # initialize Random state\n",
    "    random_state = np.random.RandomState(random_state)\n",
    "\n",
    "    # Load training and validation\n",
    "    # features and labels. Sometimes\n",
    "    # we want to train on input subsets,\n",
    "    # this will achieve that.\n",
    "    X_train = train_data[\"features\"]\n",
    "    X_train = X_train[:, 0:input_size, :]\n",
    "    y_train = train_data[\"labels\"]\n",
    "\n",
    "    if train_sample:\n",
    "        rand_val = np.random.choice(\n",
    "            np.arange(len(y_train), dtype=int),\n",
    "            size=(int(train_sample_fraction * len(y_train)),),\n",
    "            replace=False,\n",
    "        )\n",
    "\n",
    "        X_train = X_train[rand_val, :, :]\n",
    "        y_train = y_train[rand_val]\n",
    "\n",
    "    # Load validation data\n",
    "    val_data = np.load(val_path, allow_pickle=True)\n",
    "\n",
    "    X_val = val_data[\"features\"]\n",
    "    X_val = X_val[:, 0:input_size, :]\n",
    "    y_val = val_data[\"labels\"]\n",
    "\n",
    "    if val_sample:\n",
    "        rand_val = np.random.choice(\n",
    "            np.arange(len(y_val), dtype=int),\n",
    "            size=(int(val_sample_fraction * len(y_val)),),\n",
    "            replace=False,\n",
    "        )\n",
    "\n",
    "        X_val = X_val[rand_val, :, :]\n",
    "        y_val = y_val[rand_val]\n",
    "\n",
    "    print(\n",
    "        f\"Training features shape {X_train.shape},\"\n",
    "        f\" training labels shape: {y_train.shape}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation features shape {X_val.shape}, \"\n",
    "        f\" validation labels shape: {y_val.shape}\"\n",
    "    )\n",
    "\n",
    "    count_pos_neg(y_train, set_name=\"Train\")\n",
    "    count_pos_neg(y_val, set_name=\"Validation\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, random_state\n",
    "\n",
    "\n",
    "def _fdr2qvalue(fdr, num_total, met, indices):\n",
    "    \"\"\"\n",
    "    Method from Will Fondrie's mokapot,\n",
    "    remember to cite:\n",
    "    https://github.com/wfondrie/mokapot\n",
    "    Quickly turn a list of FDRs to q-values.\n",
    "    All of the inputs are assumed to be sorted.\n",
    "    :param fdr: numpy.ndarray, A vector of all\n",
    "                         unique FDR values.\n",
    "    :param num_total: numpy.ndarray, A vector of\n",
    "                               the cumulative\n",
    "                               number of PSMs\n",
    "                               at each score.\n",
    "    :param met: numpy.ndarray, A vector of the scores\n",
    "                         for each PSM.\n",
    "    :param indices: tuple of numpy.ndarray,\n",
    "                        Tuple where the vector\n",
    "                        at index i indicates\n",
    "                        the PSMs that shared\n",
    "                        the unique FDR value in `fdr`.\n",
    "    :return: numpy.ndarray, A vector of q-values.\n",
    "    \"\"\"\n",
    "    min_q = 1\n",
    "    qvals = np.ones(len(fdr))\n",
    "    group_fdr = np.ones(len(fdr))\n",
    "    prev_idx = 0\n",
    "    for idx in range(met.shape[0]):\n",
    "        next_idx = prev_idx + indices[idx]\n",
    "        group = slice(prev_idx, next_idx)\n",
    "        prev_idx = next_idx\n",
    "\n",
    "        fdr_group = fdr[group]\n",
    "        n_group = num_total[group]\n",
    "        curr_fdr = fdr_group[np.argmax(n_group)]\n",
    "        if curr_fdr < min_q:\n",
    "            min_q = curr_fdr\n",
    "\n",
    "        group_fdr[group] = curr_fdr\n",
    "        qvals[group] = min_q\n",
    "\n",
    "    return qvals\n",
    "\n",
    "\n",
    "def tdc(scores, target, pn_ratio=2, desc=True):\n",
    "    \"\"\"\n",
    "    Method from Will Fondrie's mokapot,\n",
    "    remember to cite:\n",
    "    https://github.com/wfondrie/mokapot\n",
    "\n",
    "    Estimate q-values using target decoy\n",
    "    competition. Estimates q-values using\n",
    "    the simple target decoy competition method.\n",
    "    For set of target and decoy PSMs meeting a\n",
    "    specified score threshold, the false discovery\n",
    "    rate (FDR) is estimated as:\n",
    "    ...math:\n",
    "        FDR = \\frac{2*Decoys}{Targets + Decoys}\n",
    "    More formally, let the scores of target and\n",
    "    decoy PSMs be indicated as\n",
    "    :math:`f_1, f_2, ..., f_{m_f}` and\n",
    "    :math:`d_1, d_2, ..., d_{m_d}`,\n",
    "    respectively. For a score threshold\n",
    "    :math:`t`, the false discovery\n",
    "    rate is estimated as:\n",
    "    ...math:\n",
    "        E\\\\{FDR(t)\\\\} = \\frac{|\\\\{d_i > t; i=1, ..., m_d\\\\}| + 1}\n",
    "        {\\\\{|f_i > t; i=1, ..., m_f|\\\\}}\n",
    "    The reported q-value for each PSM is the\n",
    "    minimum FDR at which that PSM would be accepted.\n",
    "\n",
    "    :param scores: numpy.ndarray of float\n",
    "        A 1D array containing the score to rank by\n",
    "    :param target : numpy.ndarray of bool\n",
    "        A 1D array indicating if the entry is from\n",
    "        a target or decoy hit. This should be boolean,\n",
    "        where `True` indicates a target and `False`\n",
    "        indicates a decoy. `target[i]` is the label\n",
    "        for `metric[i]`; thus `target` and `metric`\n",
    "        should be of equal length.\n",
    "    :param pn_ratio: float, ratio of positive/negative\n",
    "                            class examples\n",
    "    :param desc : bool Are higher scores better?\n",
    "                      `True` indicates that they are,\n",
    "                      `False` indicates that they are not.\n",
    "\n",
    "    :returns: numpy.ndarray\n",
    "        A 1D array with the estimated q-value for each entry.\n",
    "        The array is the same length as the `scores` and\n",
    "        `target` arrays.\n",
    "    \"\"\"\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    try:\n",
    "        target = np.array(target, dtype=bool)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"'target' should be boolean.\")\n",
    "\n",
    "    if scores.shape[0] != target.shape[0]:\n",
    "        raise ValueError(\"'scores' and 'target' must be the same length\")\n",
    "\n",
    "    # Unsigned integers can cause weird things to happen.\n",
    "    # Convert all scores to floats to for safety.\n",
    "    if np.issubdtype(scores.dtype, np.integer):\n",
    "        scores = scores.astype(np.float_)\n",
    "\n",
    "    # Sort and estimate FDR\n",
    "    if desc:\n",
    "        srt_idx = np.argsort(-scores)\n",
    "    else:\n",
    "        srt_idx = np.argsort(scores)\n",
    "\n",
    "    scores = scores[srt_idx]\n",
    "    target = target[srt_idx]\n",
    "\n",
    "    cum_targets = target.cumsum()\n",
    "\n",
    "    cum_decoys = ((target - 1) ** 2).cumsum()\n",
    "    num_total = cum_targets + cum_decoys\n",
    "\n",
    "    # Handles zeros in denominator\n",
    "    fdr = np.divide(\n",
    "        (1 + pn_ratio) * cum_decoys,\n",
    "        (cum_targets + cum_decoys),\n",
    "        out=np.ones_like(cum_targets, dtype=float),\n",
    "        where=(cum_targets != 0),\n",
    "    )\n",
    "\n",
    "    # Calculate q-values\n",
    "    unique_metric, indices = np.unique(scores, return_counts=True)\n",
    "\n",
    "    # Some arrays need to be flipped\n",
    "    # so that we can loop through from\n",
    "    # worse to best score.\n",
    "    fdr = np.flip(fdr)\n",
    "    num_total = np.flip(num_total)\n",
    "    if not desc:\n",
    "        unique_metric = np.flip(unique_metric)\n",
    "        indices = np.flip(indices)\n",
    "\n",
    "    qvals = _fdr2qvalue(fdr, num_total, unique_metric, indices)\n",
    "    qvals = np.flip(qvals)\n",
    "    qvals = qvals[np.argsort(srt_idx)]\n",
    "\n",
    "    return qvals\n",
    "\n",
    "\n",
    "def compute_pos_neg_sets(x_data, scores, y_data, score_threshold):\n",
    "    \"\"\"\n",
    "    Compute positive and negative sets using\n",
    "    the validation score threshold.\n",
    "    :param x_data: np.array, features are\n",
    "                             stored here.\n",
    "    :param scores: np.array, CNN scores\n",
    "                             for data.\n",
    "    :param y_data: np.array, m6A labels.\n",
    "    :param score_threshold: float, score threshold\n",
    "                                   based on validation\n",
    "                                   data.\n",
    "    :return: new features, new labels and all labels.\n",
    "    \"\"\"\n",
    "    pos_set_score = np.where(scores >= score_threshold)[0] # indices samples with score above thresh\n",
    "\n",
    "    pos_set_all = np.where(y_data == 1)[0] # indices of pos samples\n",
    "\n",
    "    pos_set = np.intersect1d(pos_set_score, pos_set_all) # pos samples with score above thresh\n",
    "\n",
    "    neg_set = np.where(y_data == 0)[0]\n",
    "\n",
    "    y_data_init = np.zeros((len(pos_set) + len(neg_set),))\n",
    "\n",
    "    y_data_init[0 : len(pos_set)] = 1 # all pos samples are labeled 1, neg samples are 0\n",
    "\n",
    "    x_data_init = np.concatenate((x_data[pos_set, :, :], x_data[neg_set, :, :])) # splits the pos and neg sets\n",
    "\n",
    "    shuffle_idx = np.arange(len(y_data_init), dtype=int)\n",
    "    np.random.shuffle(shuffle_idx)\n",
    "\n",
    "    x_data_init = x_data_init[shuffle_idx, :, :] # random subset of data\n",
    "    y_data_init = y_data_init[shuffle_idx]\n",
    "\n",
    "    count_pos_neg(y_data_init, set_name=\"New training set\")\n",
    "    return x_data_init, y_data_init\n",
    "\n",
    "\n",
    "def compute_fdr_score(scores, y_data, fdr_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Compute FDR score threshold using the validation data.\n",
    "    :param scores: np.array, array of CNN scores\n",
    "    :param y_data: np.array, ground truth labels\n",
    "    :param fdr_threshold: float, false discovery rate\n",
    "    :return:score_thresholds and number of positives\n",
    "    \"\"\"\n",
    "    num_pos_class = len(np.where(y_data == 1)[0])\n",
    "    num_neg_class = len(np.where(y_data == 0)[0])\n",
    "    # ratio of positives to negatives\n",
    "    pn_ratio = num_pos_class / float(num_neg_class)\n",
    "    if verbose:\n",
    "        print(f\"positive class to negative\" f\" class ratio: {pn_ratio}\")\n",
    "\n",
    "    ipd_fdr = tdc(scores, y_data, pn_ratio, desc=True) # gets q-values from scores\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"ipd_fdr: min: {np.min(ipd_fdr)}, \"\n",
    "            f\"max: {np.max(ipd_fdr)}, \"\n",
    "            f\"mean: {np.mean(ipd_fdr)}, \"\n",
    "            f\"median: {np.median(ipd_fdr)}, \"\n",
    "            f\"std: {np.std(ipd_fdr)}\"\n",
    "        )\n",
    "\n",
    "    # Get positive set\n",
    "    # from samples with\n",
    "    # positive label\n",
    "    # and FDR below the\n",
    "    # threshold.\n",
    "    pos_set = np.where(ipd_fdr <= fdr_threshold)[0] # indices where fdr is below thresh\n",
    "\n",
    "    # If we found no samples,\n",
    "    # relax FRD criteria\n",
    "    # to get an initial\n",
    "    # positive set.\n",
    "    if len(pos_set) == 0:\n",
    "        pos_set = np.where(ipd_fdr <= np.min(ipd_fdr))[0]\n",
    "\n",
    "    if torch.is_tensor(pos_set):\n",
    "        pos_set = pos_set.numpy()\n",
    "\n",
    "    if torch.is_tensor(scores):\n",
    "        scores = scores.numpy()\n",
    "\n",
    "    pos_scores = scores[pos_set]\n",
    "    # Get the score threshold\n",
    "    # for positive examples\n",
    "    # which are below a certain\n",
    "    # threshold.\n",
    "    score_thresholds = np.min(pos_scores)\n",
    "\n",
    "    # Number of positives\n",
    "    # below the FDR threshold\n",
    "    num_pos = len(pos_scores)\n",
    "\n",
    "    return score_thresholds, num_pos\n",
    "\n",
    "\n",
    "def run():\n",
    "    # read parameters from config file\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"../config.yml\")\n",
    "\n",
    "    # get parameters for the relevant chemistry\n",
    "    rel_config = config[\"train_ONT_chemistry\"]\n",
    "    # Number of input channels\n",
    "    input_size = int(rel_config[\"input_size\"])\n",
    "    # length of input sequence\n",
    "    input_length = int(rel_config[\"input_length\"])\n",
    "    # path to training data set\n",
    "    train_data = \"../data/ml_data/HG002_2_3_00_400k_train.npz\"\n",
    "    # path to validation data set\n",
    "    val_data = \"../data/ml_data/HG002_2_3_00_400k_val.npz\"\n",
    "    # cpu or cuda for training\n",
    "    device = rel_config[\"device\"]\n",
    "    # path to pretrained supervised model\n",
    "    best_sup_save_model = rel_config[\"pretrain_model_name\"]\n",
    "    # path to save best model\n",
    "    best_save_model = rel_config[\"best_semi_supervised_model_name\"]\n",
    "    # path to save final model\n",
    "    final_save_model = rel_config[\"final_semi_supervised_model_name\"]\n",
    "    # maximum number of epochs for training\n",
    "    max_epochs = 5\n",
    "    # FDR threshold for semi-supervised training\n",
    "    fdr_threshold = float(rel_config[\"fdr\"])\n",
    "    # save training results\n",
    "    save_pos = rel_config[\"save_pos\"]\n",
    "    # number of threads to process training data fetch\n",
    "    num_workers = int(rel_config[\"semi_num_workers\"])\n",
    "    # batch size of training data\n",
    "    batch_size = int(rel_config[\"semi_batch_size\"])\n",
    "    # Boolean value to indicate if we want to subsample training data\n",
    "    train_sample = int(rel_config[\"train_sample\"])==1\n",
    "    # fraction of training data to sample\n",
    "    train_sample_fraction = float(rel_config[\"train_sample_fraction\"])\n",
    "    # Boolean value to indicate if we want to subsample validation data\n",
    "    val_sample = int(rel_config[\"val_sample\"])==1\n",
    "    # fraction of validation data to sample\n",
    "    val_sample_fraction = float(rel_config[\"val_sample_fraction\"])\n",
    "    # learning rate\n",
    "    semi_lr = float(rel_config[\"semi_lr\"])\n",
    "    # If we did sample, ensure the sample has at this fraction\n",
    "    # of m6A calls above FDR < 5%, otherwise resample\n",
    "    # upto 5 times. \n",
    "    min_pos_proportion = float(rel_config[\"min_pos_proportion\"])\n",
    "    # Used in the convergence criteria\n",
    "    # percent additional m6As identified from the validation set\n",
    "    # in the current iteration\n",
    "    add_m6a_percent = float(rel_config[\"add_m6a_percent\"])\n",
    "    # percent total m6A identified from the validation set \n",
    "    # so far\n",
    "    total_m6a_percent = float(rel_config[\"total_m6a_percent\"])\n",
    "\n",
    "\n",
    "    # Load the supervised model for transfer learning\n",
    "    #model = M6ANet()\n",
    "    model = M6ALstm()\n",
    "    with open(best_sup_save_model, \"rb\") as fp:\n",
    "        #model.load_state_dict(pickle.load(fp))\n",
    "        contents = CPU_Unpickler(fp).load()\n",
    "    \n",
    "    #model = model.to(device)\n",
    "\n",
    "    # Adam optimizer with learning\n",
    "    # rate 1e-4\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=semi_lr)\n",
    "\n",
    "    # Print model architecture summary\n",
    "    #summary_str = summary(model, input_size=(input_size, input_length))\n",
    "\n",
    "    # keep track of validation set's\n",
    "    # number of positives below FDR < 0.05\n",
    "    # precision in supervised setting\n",
    "    # and score threshold for validation\n",
    "    # FDR <= 0.05\n",
    "    all_num_pos = []\n",
    "    val_ap = []\n",
    "    val_scores = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    # If the initial validation\n",
    "    # set is sampled, we need\n",
    "    # to make sure that the\n",
    "    # sample has at least 100\n",
    "    # m6A calls below the\n",
    "    # FDR threshold\n",
    "    regenerate = True\n",
    "    # to avoid infinite\n",
    "    # repetitions, introduce\n",
    "    # num_repeat, which will\n",
    "    # give the while loop 5\n",
    "    # chances to generate good\n",
    "    # intial sample, if desired\n",
    "    # sample is not generated by\n",
    "    # the end of fifth iteration,\n",
    "    # proceed with the given\n",
    "    # set with a warning.\n",
    "    num_repeat = 0\n",
    "    while regenerate:\n",
    "        # Get training data generator\n",
    "        # and validation data.\n",
    "        gen_outputs = m6AGenerator(\n",
    "            train_data,\n",
    "            val_data,\n",
    "            input_size=input_size,\n",
    "            random_state=None,\n",
    "            train_sample=train_sample,\n",
    "            train_sample_fraction=train_sample_fraction,\n",
    "            val_sample=val_sample,\n",
    "            val_sample_fraction=val_sample_fraction,\n",
    "        )\n",
    "\n",
    "        X_train, y_train, X_val, y_val, random_state = gen_outputs\n",
    "\n",
    "        # Use m6a call quality score\n",
    "        # for the central base as\n",
    "        # the initial classifier\n",
    "        y_score_val = X_val[:, 5, 7]\n",
    "\n",
    "        # Convert y_val to\n",
    "        # a one-hot-encoded\n",
    "        # vector\n",
    "        y_val_ohe = make_one_hot_encoded(y_val) # what is the purpose of this?\n",
    "\n",
    "        # Compute initial score threshold\n",
    "        # for the m6a call score\n",
    "        # classifier using the FDR <= 0.05\n",
    "        # critera and the validation data\n",
    "        score_threshold, num_pos = compute_fdr_score(\n",
    "            y_score_val, np.array(y_val, dtype=bool), fdr_threshold=fdr_threshold\n",
    "        )\n",
    "        pos_proportion = float(num_pos) / len(y_score_val)\n",
    "        if pos_proportion >= min_pos_proportion:\n",
    "            regenerate = False\n",
    "        elif val_sample and pos_proportion < min_pos_proportion:\n",
    "            if num_repeat < 5:\n",
    "                regenerate = True\n",
    "                print(\n",
    "                    f\"Number of positives sampled at FDR {fdr_threshold} is {num_pos},\"\n",
    "                    f\" needs to be >= {int(len(y_score_val)*min_pos_proportion)}, \"\n",
    "                    f\" sampling again.\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Number of positives sampled at FDR {fdr_threshold} is {num_pos},\"\n",
    "                    f\" needs to be >= {int(len(y_score_val)*min_pos_proportion)} \"\n",
    "                    f\" optimization may not converge.\"\n",
    "                )\n",
    "                regenerate = False\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                f\"Number of positives at FDR {fdr_threshold} is {num_pos},\"\n",
    "                f\" needs to be >= {int(len(y_score_val)*min_pos_proportion)} \"\n",
    "                f\" optimization may not converge.\"\n",
    "            )\n",
    "            regenerate = False\n",
    "        num_repeat += 1\n",
    "\n",
    "    # store the number of\n",
    "    # positives identified\n",
    "    # and the score theshold\n",
    "    all_num_pos.append(num_pos)\n",
    "    val_scores.append(score_threshold)\n",
    "\n",
    "    # Compute the average precision\n",
    "    # of the initial inter-pulse\n",
    "    # distance classifier\n",
    "    ap_calculator = AUC(curve='PR')\n",
    "    ap_calculator.update_state(y_val, y_score_val)\n",
    "    tf_ap = ap_calculator.result().numpy()\n",
    "    #sklearn_ap = average_precision_score(y_val, y_score_val)\n",
    "\n",
    "    accuracy_calculator = Accuracy()\n",
    "    accuracy_calculator.update_state(y_val, y_score_val)\n",
    "    tf_accuracy = accuracy_calculator.result().numpy()\n",
    "\n",
    "    # Compute total number of positives\n",
    "    # and negatives in the validation set\n",
    "    val_pos_all, val_neg_all = count_pos_neg(y_val, set_name=\"Validation set\")\n",
    "    print(f\"Validation set has {val_pos_all} positives and {val_neg_all} negatives\")\n",
    "    print(\n",
    "        f\"Validation IPD average precision: \"\n",
    "        f\"{tf_ap}, Number of positives \"\n",
    "        f\" at FDR of {fdr_threshold} are: {num_pos}\"\n",
    "    )\n",
    "\n",
    "    # store the initial\n",
    "    # average precision and accuracy\n",
    "    val_ap.append(tf_ap)\n",
    "    val_accuracy.append(tf_accuracy)\n",
    "\n",
    "    # Use m6a call quality score\n",
    "    # for the central base as\n",
    "    # the initial classifier\n",
    "    y_score = X_train[:, 5, 7]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"y_score: {y_score.shape}, \"\n",
    "            f\"min: {np.min(y_score)}, \"\n",
    "            f\"max: {np.max(y_score)}, \"\n",
    "            f\"mean: {np.mean(y_score)}, \"\n",
    "            f\"std: {np.std(y_score)}\"\n",
    "        )\n",
    "\n",
    "    # Get initial training set using initial\n",
    "    # score determined using IPD classifier\n",
    "    # on validation data.\n",
    "    X_init, y_init = compute_pos_neg_sets(\n",
    "        X_train, y_score, np.array(y_train, dtype=bool), score_threshold\n",
    "    )\n",
    "\n",
    "    # Convert y_init to a one-hot-encoded\n",
    "    # vector\n",
    "    y_init_ohe = make_one_hot_encoded(y_init)\n",
    "\n",
    "    # keep all training data and\n",
    "    # all validation data for\n",
    "    # precision computations\n",
    "    X_init_cpu = torch.tensor(X_train)\n",
    "    X_init_cpu = X_init_cpu.float()\n",
    "    X_val_cpu = torch.tensor(X_val)\n",
    "    X_val_cpu = X_val_cpu.float()\n",
    "\n",
    "    # Begin the semi-supervised\n",
    "    # loop\n",
    "    for i in range(max_epochs):\n",
    "\n",
    "        # print(val_features.shape)\n",
    "        # print(val_labels.shape)\n",
    "        model.fit_semisupervised(\n",
    "            train_features=X_train,\n",
    "            train_labels=y_train,\n",
    "            val_features=X_val,\n",
    "            val_labels=y_val, # shd this be ohe?\n",
    "            max_epochs=5,\n",
    "            device=\"cpu\",\n",
    "            best_save_model=best_save_model,\n",
    "            final_save_model=final_save_model,\n",
    "            prev_aupr=tf_ap\n",
    "        )\n",
    "\n",
    "\n",
    "        accuracy, precision, recall, tf_ap = model.evaluate(X_val, y_val)\n",
    "        # Get current model's AUPR\n",
    "        # on the validation data\n",
    "        val_ap.append(tf_ap)\n",
    "        val_accuracy.append(tf_ap)\n",
    "\n",
    "        # Generate scores for\n",
    "        # the validation data\n",
    "        # using the current model\n",
    "        y_score_val = model.predict(X_val_cpu)\n",
    "\n",
    "        # Compute new score threshold\n",
    "        # using the new validation\n",
    "        # set predictions\n",
    "        score_threshold, num_pos = compute_fdr_score(\n",
    "            y_score_val[:, 0], np.array(y_val, dtype=bool), fdr_threshold=fdr_threshold\n",
    "        )\n",
    "\n",
    "        # Store the number of positives in\n",
    "        # the validation set and score threshold\n",
    "        # for the positives below FDR <= 0.05.\n",
    "        all_num_pos.append(num_pos)\n",
    "        val_scores.append(score_threshold)\n",
    "\n",
    "        print(\n",
    "            f\"Validation CNN epoch {i}\"\n",
    "            f\" average precision: {tf_ap}, \"\n",
    "            f\" Number of positives at estimated precision of \"\n",
    "            f\"{1.0-fdr_threshold} are: {num_pos}\"\n",
    "        )\n",
    "\n",
    "        # Generate scores for\n",
    "        # all the training data\n",
    "        y_score = model.predict(X_init_cpu)\n",
    "\n",
    "        # Compute positive and negative\n",
    "        # sets using the validation set\n",
    "        # determined score_threshold\n",
    "        # and training set data\n",
    "        X_init, y_init = compute_pos_neg_sets(\n",
    "            X_train, y_score[:, 0], np.array(y_train, dtype=bool), score_threshold\n",
    "        )\n",
    "\n",
    "        # Convert y_init to a one-hot-encoded\n",
    "        # vector\n",
    "        y_init_ohe = make_one_hot_encoded(y_init)\n",
    "\n",
    "        # Store the total positives\n",
    "        # in the validation set,\n",
    "        # and lists of validation\n",
    "        # precision and number of\n",
    "        # positives identified as\n",
    "        # training progressed.\n",
    "        np.savez(\n",
    "            save_pos,\n",
    "            total_pos=val_pos_all,\n",
    "            num_pos=all_num_pos,\n",
    "            val_ap=val_ap,\n",
    "            val_score=val_scores,\n",
    "        )\n",
    "        \n",
    "\n",
    "        additional_pos = num_pos - all_num_pos[-2]\n",
    "        add_pos_percent = (additional_pos / float(val_pos_all)) * 100\n",
    "        num_pos_identified = (num_pos / float(val_pos_all)) * 100\n",
    "        \n",
    "        # If more than total_m6a_percent m6As have been found and the increase in \n",
    "        # additional m6As found is less than add_m6a_percent, then stop training.\n",
    "        if add_pos_percent < add_m6a_percent and num_pos_identified > total_m6a_percent:\n",
    "            print(\n",
    "                f\"New identified m6A are {add_pos_percent:.2f}% (less than 1%),\"\n",
    "                f\" and total number of m6A identified are {num_pos_identified:.2f}%.\"\n",
    "                f\" Convergence reached, stopping training. \"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    \n",
    "    return max_epochs, val_ap, val_scores\n",
    "\n",
    "\n",
    "epochs, val_ap, val_scores = run()\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Train & Val Accuracy')\n",
    "plt.plot(range(0, epochs + 1), val_ap, label='Train')\n",
    "plt.plot(range(0, epochs + 1), val_scores, label='Val')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
